\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{hyperref}

\title{Optimizing DTW-Based Audio-to-MIDI Alignment and Matching}

\name{Colin Raffel and Daniel P. W. Ellis}
\address{LabROSA\\
    Columbia University\\
    New York, NY}

\begin{document}

\maketitle

\begin{abstract}
Dynamic Time Warping (DTW) has proven to be an extremely effective method for both aligning and matching recordings of songs to corresponding MIDI transcriptions.
The performance of DTW-based approaches in this domain is heavily effected by system design choices, such as the representation used for the audio and MIDI data and DTW's adjustable hyperparameters.
We propose a method for optimizing the design of DTW-based alignment and matching systems.
Our technique uses Bayesian optimization to tune system design and hyperparameters over a synthetically created dataset of audio and MIDI pairs.
We then perform an exhaustive search over DTW score normalization techniques in order to determine an optimal method for reporting a reliable alignment confidence score, which is necessary for matching tasks.
Using our approach, we are able to create a DTW-based system which is conceptually simple and highly accurate at both alignment and matching.
We also verified that our system achieves high performance in a large-scale qualitative evaluation of results on real-world data.
\end{abstract}

\begin{keywords}
Dynamic Time Warping, Audio to MIDI Alignment, Sequence Retrieval, Bayesian Optimization, Hyperparameter Optimization
\end{keywords}

\section{Introduction}
\label{sec:intro}

MIDI files can provide a bounty of ground-truth data for content-based music information retrieval (MIR) tasks, including beat/bar tracking, onset detection, key estimation, automatic transcription, and score-informed source separation \cite{ewert2012towards, turetsky2003ground, ewert2014score, raffel2014pretty_midi}.
However, in order to utilize this data, a given MIDI file must first be aligned in time to a recording of the piece it is a transcription of.
A related problem is MIDI-to-audio matching, where we are given separate collections of MIDI and audio files and must determine which audio file (if any) each MIDI file corresponds to.
This problem is motivated by the dearth of metadata information typically available in MIDI files, making any kind of text-based matching infeasible \cite{raffel2015large}.
With or without useful metadata, it is beneficial to be able to produce a confidence score for alignment which communicates how well the content in a MIDI file matches a given audio file, which can help determine the transcription quality.

Despite these tasks' complementary nature, most previous research has focused on systems meant for either alignment or matching.
In the context of MIDI-to-audio alignment, a wide variety of techniques have been used, which typically involve determining a correspondence between discrete times in the audio and MIDI file.
While approaches inspired by edit distance metrics such as Smith-Waterman \cite{ewert2012towards} and Needleman-Wunsch \cite{grachten2013automatic} have been used, arguably the most common approach is Dynamic Time Warping (DTW) \cite{muller2007dynamic}.
First proposed for comparing speech utterances \cite{sakoe1978dynamic}, DTW uses dynamic programming to find a monotonic alignment such that the total distance between aligned feature vectors is minimized.
This property makes it well-suited for audio-to-MIDI alignment when we expect that the MIDI is an accurate continuous transcription (i.e.\ without out-of-sequence or incorrect sections).

A nice property of DTW is its appropriateness as a measure of the ``similarity'' between two sequences.
This is thanks to the fact that the total distance between pairs of feature vectors in the DTW-based alignment can be used reliability as a distance metric.
In fact, DTW has seen extensive use solely as a way of measuring sequence similarity in the data mining literature \cite{berndt1994using}.
In the context of MIDI and audio files, \cite{hu2003polyphonic} evaluated the effectiveness DTW distance to match a small collection of Beatles MIDIs to recordings of Beatles songs.

Despite its widespread use, DTW's success can be highly dependent on its exact formulation as well as system design choices such as the feature representation and local distance metric used.
To our knowledge, there has been no large-scale quantitative comparison of different DTW-based alignment systems.
This is likely due to the fact that evaluating its performance would require either a collection of MIDI and audio pairs for which the correct alignment is already known (which does not exist) or manual audition and rating of the output of the system (which is time-consuming).
The present work aims to remedy this and produce a DTW-based alignment system which is optimal in terms of both alignment and matching accuracy.

After giving an overview of typical DTW-based alignment systems (Section \ref{sec:dtw}), we propose a method for creating a synthetic dataset of MIDI-audio pairs by applying realistic corruptions to MIDI files, which allows us to know a priori the correct alignment (Section \ref{sec:synthetic}).
We then tune hyperparameters for alignment using Bayesian optimization (Section \ref{sec:optimizing}) and for confidence reporting using an exhaustive search (Section \ref{sec:confidence}).
Finally, we perform a large-scale qualitative evaluation of our proposed alignment system on real-world data (Section \ref{sec:qualitative}) and discuss possible avenues for improvement (Section \ref{sec:avenues}).

\section{DTW-Based Alignment}
\label{sec:dtw}

Suppose we are given two sequences of feature vectors $X \in \mathbb{R}^{M \times D}$ and $Y \in \mathbb{R}^{N \times D}$, where $D$ is the feature dimensionality and $M$ and $N$ are the number of feature vectors in $X$ and $Y$ respectively.
Dynamic Time Warping produces two sequences $p, q \in \mathbb{N}^L$ which define the optimal alignment between $X$ and $Y$, such that $p[i] = n, q[i] = m$ implies that the $m$th feature vector in $X$ should be aligned to the $n$th in $Y$.
In DTW, finding $p$ and $q$ involves solving the following minimization problem:
$$
p, q = \mathrm{arg}\min_{p, q} \sum_{i = 1}^{L} \|X[p[i]] - Y[q[i]]\|_2^2 + \Phi(i)
$$
where $\Phi(i)$ is $\phi$ when $p[i] = p[i - 1]$ or $q[i] = q[i - 1]$ and $0$ otherwise.
$\phi \ge 0$ is a constant which is used to discourage ``non-diagonal moves'', i.e.\ indices in the path where one feature vector in one sequence is mapped to multiple vectors in the other.
This minimization problem can be solved in $\mathcal{O}(MN)$ time using dynamic programming \cite{sakoe1978dynamic}.
Once $p$ and $q$ are found, the original MIDI file can be adjusted so that events which occur at $t_X[p[i]]$ are moved to $t_Y[q[i]]$, where $t_X \in \mathbb{R}^M, t_Y \in \mathbb{R}^N$ are the times corresponding to the feature vectors in $X$ and $Y$ respectively.

Traditionally, DTW is constrained such that $p$ and $q$ span the entirety of $X$ and $Y$, i.e.\ $p[1] = q[1] = 1$ and $p[L] = N; q[L] = M$.
However, audio-to-MIDI alignment typically allows subsequence matching (a desireable case, for example, when a MIDI file is a transcription of a portion of a song), where instead we only require that either $gN \le p[L] \le N$ or $gM \le q[L] \le M$.
$g \in [0, 1]$ (the ``gully'') is a parameter which determines the proportion of the subsequence which must be successfully matched.
In addition, the path is occasionally further constrained so that
\begin{equation}
q[i] - p[i] + R \le N, p[i] - q[i] + R \le M
\end{equation}
for $i \in \{1, \ldots, L\}$ where $R = g\min(M, N)$ is the ``radius'', sometimes called the Sakoe-Chiba band \cite{sakoe1978dynamic}.
A further constraint on $p$ and $q$ is monotonicity, i.e.\ that
$$
p[i + 1] \ge p[i], q[i + 1] \ge q[i] \; \forall i
$$
This can be enforced by allowing a specific ``step pattern''; while many exist \cite{muller2007dynamic, sakoe1978dynamic}, to our knowledge the only pattern used in audio to MIDI alignment simply requires that
$$
(p[i + 1], q[i + 1]) \in \begin{cases}
(p[i] + 1, q[i] + 1)\\
(p[i], q[i] + 1)\\
(p[i] + 1, q[i])
\end{cases}
$$

From the above definition, it is clear that there are many design choices to be made when performing DTW-based audio-to-MIDI alignment:
\begin{description}[topsep=1pt,itemsep=-1pt,leftmargin=5pt]
\item[Feature representation ($X$ and $Y$):] Prior to alignment, audio and MIDI sequences must be converted to a common mid-level representation.
This is typically done by synthesizing the MIDI file to obtain an audio signal and computing a common spectral transform of the audio recording and synthesized MIDI audio.
Chroma vectors, which represent the amount of energy in each semitone summed across octaves \cite{fujishima1999realtime}, are a common choice \cite{hu2003polyphonic, ewert2012towards}.
A constant-Q transform (CQT), which represents the amount of energy in logarithmically spaced bins \cite{brown1991calculation} has also been used \cite{raffel2015large, dixon2005match, ellis2013aligning}.
Occasionally, log-magnitude features are used in order to more closely mimic human perception \cite{raffel2015large, ellis2013aligning, turetsky2003ground}.
In \cite{turetsky2003ground} and \cite{hu2003polyphonic} it is noted that Mel-Frequency Cepstral Coefficients result in poor performance for music signals because they discard pitch information.
\item[Time scale ($t_X$ and $t_Y$):] Feature vectors are frequently computed over short, overlapping frames of audio \cite{dixon2005match, turetsky2003ground, hu2003polyphonic}.
Note that the spacing between feature vectors must be sufficiently small compared to the auditory temporal resolution, e.g.\ tens of milliseconds \cite{blauert1997spatial}.
Occasionally, beat-synchronous feature vectors are used \cite{raffel2015large,ellis2013aligning}, which can dramatically reduce computation time and can produce perceptually accurate alignments provided that the beat tracking is correct.
\item[Normalization:] In \cite{rakthanmanon2012searching}, it is argued that z-scoring (standardizing) the feature sequences is critical for data mining applications of DTW, which was used in audio-to-MIDI alignment in \cite{hu2003polyphonic}.
In addition, various normalizations have been applied to the feature vectors in $X$ and $Y$ before computing their local distances.
A common choice is normalizing by each vector by its $L^2$ norm, which effectively results in the local distance being the cosine distance \cite{turetsky2003ground, ewert2012towards, raffel2015large, ellis2013aligning}.
\item[Penalty ($\phi$):] In many applications of DTW to audio-to-MIDI alignment, no additive penalty is used, which corresponds to setting $\phi = 0$.
However, as long as the MIDI and audio files have consistent tempi, non-diagonal moves should be discouraged.
In addition, it has been argued \cite{raffel2015large} that when the constraint $p[1] = 1, q[1] = 1$ is not enforced (which allows subsequence alignment), an additive penalty can be crucial to ensure that the entire subsequence is used, and so $\phi$ is set to the 90th percentile of the distances $\|X[n] - Y[m]\|_2^2$ for $m \in \{1, \ldots, M\}; n \in \{1, \ldots, N\}$.
In \cite{ellis2013aligning}, a fixed value of $\phi = .5$ is used.
\item[Gully ($g$) and band path constraint:] As with $\phi$, the ``gully'' and band path constraint are often omitted, which corresponds to setting $g = 1$.
A value of $g$ which is close to 1 will afford some tolerance to the possibility that the beginning or end of the MIDI transcription is incorrect (e.g.\ a fade-out or lead-in), so \cite{ellis2013aligning} sets $g = .7$ and \cite{raffel2015large} sets $g = .95$.
In data mining applications \cite{ratanamahatana2004everything} it is argued that the band radius path constraint both reduces computational complexity and results in more reliable alignments by avoiding paths with many of non-diagonal moves.
\end{description}

\section{Creating a Synthetic Alignment Dataset}
\label{sec:synthetic}

From the discussion above, it is clear that there is a large space of hyperparameters (design choices) for DTW-based audio-to-MIDI alignment systems.
To our knowledge, previous researchers have mainly manually tuned alignment hyperparameters based on a small test set of MIDI/audio pairs and have chosen the system which gives good qualitative results by audition of the aligned MIDI data.
Because this method only facilitates small-scale comparisons, there is an obvious question of what hyperparameter settings would yield the best general-purpose alignment system.
Unfortunately, manual audition of even a tiny subset of possible hyperparameter settings on a modestly-sized collection of paired MIDI and audio files is infeasible, let alone a collection large enough to make substantive judgements about the general performance of a given system.
Furthermore, automatic evaluation has not been an option due to the lack of a ground-truth dataset of ``correct'' alignments.
We therefore propose a method for synthetically creating a collection of MIDI/audio pairs by applying a series of corruptions to MIDI files which mimic real-world data.
When applying the corruptions, we keep track of the correct adjustments needed to restore the corrupted MIDIs back to the correct timebase, which facilitates automatic comparison and allows us to rapidly and automatically a huge number of possible systems.

To create this dataset, we first collected 1,000 MIDI files which were transcriptions of Western popular music songs.
We then applied the following series of transformations, based on our experience with common differences between MIDI transcriptions and audio recordings: 
First, to simulate differences in tempo, we adjusted the timing in each MIDI file by a low-frequency length-$N$ random signal, generated as
$$
r[n] = \sum_{k = 0}^{N - 1} \mathcal{N}(0, \sigma_t) e^{j2\pi kn/N - n}, n \in \{1, \ldots, N\}
$$
i.e.\ the inverse Fourier transform of an exponentially decaying Gaussian-distributed random spectra with standard deviation $\sigma_r$.
Second, the first 10\% and last 10\% of the transcription were each cropped out with probability 50\%, which simulates the MIDI file being an incomplete transcription.
In addition, 1\% of the transcription was cut out at a random location with 10\% probability, which simulates a missing measure.
Third, because it is common for a MIDI transcription to be missing an instrument (for example, karaoke versions of songs are frequently distributed as MIDI files, which do not contain a vocal instrument), we removed each instrument track in each MIDI file with probability $p_r$, making sure to never remove all instruments.
Fourth, in all MIDI files, we randomly added 1 or -1 to the program number of each instrument.
This simulated the fact that when comparing a synthesized MIDI file to an audio recording, the timbre of a synthesized MIDI instrument is always somewhat different from its real-world counterpart.
Finally, for each note in each instrument, we multiplied the velocity by a random number sampled from $\mathcal{N}(1, \sigma_v)$ while keeping it in the MIDI velocity range $[1, 127]$.
This was meant to further simulate differences in instrument characteristics in real-world vs. synthesized songs, and also simulated missing notes for large $\sigma_v$.
All MIDI data manipulations were realized with \texttt{pretty\char`_midi} \cite{raffel2014pretty_midi}.

As described in Section \ref{sec:intro}, a DTW-based alignment system can serve two purposes: First, to align a MIDI transcription in time to an audio recording, and second, to produce a confidence score denoting the quality of the transcription or whether the MIDI file is a transcription of the recording at all.
We therefore produced two sets of corrupted version of the 1,000 MIDI files we collected, one each to measure the performance for alignment and confidence reporting.
For the first (``easy'') set, we focused on corruption parameters corresponding to real-world conditions for a high-quality transcription, setting $\sigma_t = 5, p_r = .1, \sigma_v = .2$.
For the second (``hard''), we set the corruption parameters so that the transcription was likely no longer valid, setting $\sigma_t = 20, p_r = .5, \sigma_v = 1$.

\section{Optimizing DTW-Based Alignment}
\label{sec:optimizing}

Given the framework described above for generating a synthetic alignment dataset, 

Short overview of Bayesian optimization

Parameter space (including multiplicative penalty)

Random trials

Discussion of best-performing aligners; also best aligner with beats

\section{Optimizing Confidence Reporting}
\label{sec:confidence}

Grid search

Statistical tests used

Choosing the best alignment scheme (with algorithm box?)

\section{Qualitative Evaluation}
\label{sec:qualitative}

Data preparation

Evaluation criteria

Results

\section{Avenues for Improvement}
\label{sec:avenues}

Augmentation with MUDA, partial alignments, robustness to missing instruments, re-training specifically on subsequences

\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
